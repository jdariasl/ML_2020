{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab5_parte2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7h9z6smhTpe"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdariasl/ML_2020/blob/master/Labs/lab5/lab5_parte2.ipynb\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "**Recuerda que una vez abierto, Da clic en \"Copiar en Drive\", de lo contrario no podras almacenar tu progreso**\n",
        "\n",
        "Nota: no olvide ir ejecutando las celdas de código de arriba hacia abajo para que no tenga errores de importación de librerías o por falta de definición de variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59PRMgRphTpf"
      },
      "source": [
        "# Laboratorio 5 - Parte 2 Máquinas de Vectores de Soporte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hptaCy7QhTpf"
      },
      "source": [
        "#configuración del laboratorio\n",
        "# Ejecuta esta celda!\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#for local \n",
        "#import sys ; sys.path.append('../commons/utils/')\n",
        "!wget https://raw.githubusercontent.com/jdariasl/ML_2020/master/Labs/commons/utils/general.py -O general.py --no-cache\n",
        "from general import configure_lab5_2\n",
        "configure_lab5_2()\n",
        "from lab5 import *\n",
        "GRADER, dataset = part_2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbFfmXl-hTpi"
      },
      "source": [
        "## Ejercicio 1: Limipiar base de datos y completar código\n",
        "\n",
        "En este ejercicio usaremos la regresión por vectores de soporte para resolver el problema de regresión de la base de datos AirQuality (https://archive.ics.uci.edu/ml/datasets/Air+Quality). Tener en cuenta que vamos a usar solo 2000 muestras.\n",
        "\n",
        "Para esta base de datos vamos a realizar una limpieza de datos. Para ello vamos a completar la siguiente función para realizar:\n",
        "1. **Remover** todos registros cuya variable objetivo es faltante (missing Value). Estos registros están marcados como -200, es decir, donde haya un valor -200 eliminaremos el registro.\n",
        "2. **imputar los valores perdidos/faltantes** en cada una de las características, camos a usar el valor medio de la característica en especifico.\n",
        "3. **Verificar** si quedaron valores faltantes\n",
        "4. ** retornar** X (12 primeras columnas) y Y(la 13 columna)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBT8aPiihTpi"
      },
      "source": [
        "# ejercicio de codigo\n",
        "def clean_data(dataset):\n",
        "    \"\"\"funcion que limpia el dataset y obtiene X y Y\"\"\"\n",
        "    \n",
        "    database = dataset.copy()\n",
        "    # identificar muetras cuya salida en un valor faltante\n",
        "    idx_to_remove = []\n",
        "    for i in range(np.size(dataset,0)):\n",
        "        if dataset[i,12] == ...:\n",
        "            idx_to_remove.append(i)\n",
        "    \n",
        "    #remover la muestras\n",
        "    database = np.delete(database,idx_to_remove,0)\n",
        "\n",
        "    print (\"\\nHay \" + str(len(idx_to_remove)) + \" valores perdidos en la variable de salida.\")\n",
        "    print (\"\\nDim de la base de datos sin las muestras con variable de salida perdido \"+ str(np.shape(database)))\n",
        "\n",
        "    ##Imputar\n",
        "    print (\"\\nProcesando imputación de valores perdidos en las características . . .\")\n",
        "    for k in range(0,np.size(database,0)):\n",
        "        for w in range(0,13):\n",
        "            if database[k,w] == ... :\n",
        "                all_feature = database[:,w]\n",
        "                ## Se imputa con la media de toda la caracteristica\n",
        "                # sin tener en cuenta las caractersiticas con valores faltantes               \n",
        "                database[k,w] = np...(all_feature[all_feature!=-200])\n",
        "\n",
        "    print (\"Imputación finalizada.\\n\")\n",
        "\n",
        "    ##Verificar\n",
        "    missed_values = False\n",
        "    for i in range(0,np.size(database,0)):\n",
        "        if -200 in database[i,:]:\n",
        "            missed_values = True\n",
        "    if(missed_values):\n",
        "        print (\"Hay valores perdidos\")\n",
        "    else:\n",
        "        print (\"No hay valores perdidos en la base de datos. Ahora se puede procesar\")\n",
        "\n",
        "    X = database[:,...]\n",
        "    Y = database[:,...]\n",
        "    return (X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_aHK7XThTpk"
      },
      "source": [
        "# ignorar los prints\n",
        "GRADER.run_test(\"ejercicio1\", clean_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_siuGCthTpm"
      },
      "source": [
        "Ahora usemos la función para tener nuestras variables X, Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJLs3jUohTpm"
      },
      "source": [
        "X,Y = clean_data(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfT-7D5AhTpo"
      },
      "source": [
        "## Ejercicio 2: Experimentar SVM para regresión\n",
        "\n",
        "Ahora vamos a crear la función para experimentar con la maquina de soporte vectorial. Para ellos vamos:\n",
        "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html. \n",
        "2. Vamos a variar tres parámetros del SVR: kernel,  gamma y el parametro de regularización.\n",
        "3. Utilizar la metodología cross-validation con 5 folds.\n",
        "4. Usar normalización de datos estandar implementada por sklearn\n",
        "5. Extraer los vectores de soporte (observe los *atributos* del modelo SVR de sklearn). Recuerde que estos atributos son accesibles, una vez el modelo es entrenado\n",
        "6. Utilizar el error cuadratico medio de sklearn.\n",
        "\n",
        "**Notas**: \n",
        "- Deberiamos poder acceder a las funciones de la libreria de sklearn directamente por el nombre sin necesidad de importarlas. Las funciones que deberios utilizar ya están precargadas en la configuración del laboratorio.\n",
        "- Llame todos los parametros de las funciones de sklearn de manera explicita. (i.e, si se quiere usar `max_iter` como parámetro para el SVR, debe crear el objeto: `SVR(max_iter = 100)`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84KNyMMuhTpp"
      },
      "source": [
        "#ejercicio de código\n",
        "def experiementarSVR(x, y, kernels, gammas,params_reg):\n",
        "    \"\"\"función que realizar experimentos sobre un SVM para regresión\n",
        "    \n",
        "    x: numpy.Array, con las caracteristicas del problema\n",
        "    y: numpy.Array, con la variable objetivo\n",
        "    kernels: List[str], lista con valores a pasar \n",
        "        a sklearn correspondiente al kernel de la SVM\n",
        "    gammas: List[float], lista con los valores a pasar a\n",
        "        sklean correspondiente el valor de los coeficientes para usar en el\n",
        "        kernel\n",
        "    params_reg: List[float], lista con los valores a a pasar a \n",
        "        sklearn para ser usados como parametro de regularización\n",
        "    \n",
        "    retorna: pd.Dataframe con las siguientes columnas:\n",
        "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
        "        - error cuadratico medio en el cojunto test (promedio de los 5 folds)\n",
        "        - intervalo de confianza del error cuadratico medio en el cojunto test \n",
        "            (desviacion estandar de los 5 folds)\n",
        "        - % de Vectores de Soporte promedio para los 5 folds (0 a 100)\n",
        "    \"\"\"\n",
        "    idx = 0\n",
        "    kf = ...(n_splits=5)\n",
        "    # crear una lista con la combinaciones de los elementos de cada list\n",
        "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
        "    resultados = pd.DataFrame()\n",
        "    \n",
        "    for params in kernels_gammas_regs:\n",
        "        kernel, gamma, param_reg = params\n",
        "        print(\"parametros usados\", params) # puede usar para ver los params\n",
        "    \n",
        "        errores_test = []\n",
        "        pct_support_vectors = []\n",
        "        for train_index, test_index in kf.split(...):\n",
        "            \n",
        "            X_train, X_test = x[train_index], x[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]  \n",
        "            # normalizar los datos\n",
        "            scaler = ...\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "            svm = ...\n",
        "            # Entrenar el modelo\n",
        "            svm....(X=..., y=...)\n",
        "            # Validación del modelo\n",
        "            ypred = svm....(X=...)\n",
        "            \n",
        "            # error y pct de vectores de soporte\n",
        "            errores_test.append(...(y_true = ..., y_pred = ...))\n",
        "            # contar muestras de entrenamiento\n",
        "            n_train = X_train.shape[0]\n",
        "            pct_vs = (.../ ... )\n",
        "            pct_support_vectors.append(pct_vs)\n",
        "        \n",
        "        resultados.loc[idx,'kernel'] = kernel\n",
        "        resultados.loc[idx,'gamma'] = gamma\n",
        "        resultados.loc[idx,'param_reg'] = param_reg\n",
        "        resultados.loc[idx,'error de prueba (promedio)'] = np...(errores_test)\n",
        "        resultados.loc[idx,'error de prueba (intervalo de confianza)'] = np...(errores_test)\n",
        "        resultados.loc[idx,'% de vectores de soporte'] = np...(pct_support_vectors)\n",
        "        idx+=1\n",
        "    return (resultados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB4bMhUhhTpr"
      },
      "source": [
        "GRADER.run_test(\"ejercicio2\", experiementarSVR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aatsj0EQhTpt"
      },
      "source": [
        "Para entrenar vamos a ignorar las dos primeras variables, estas corresponden a valores de fechas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRHwQhR7hTpu"
      },
      "source": [
        "# vamos a realizar los experimentos\n",
        "resultadosSVR = experiementarSVR(x =X[:,2:],y=Y,\n",
        "                                 kernels=['linear', 'rbf'],\n",
        "                                 gammas = [0.01,0.1],\n",
        "                                 params_reg = [0.1, 1.0,10]\n",
        "                                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEPmKLZhhTpw"
      },
      "source": [
        "resultadosSVR.sort_values('error de prueba (promedio)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qERAELnQhTpx"
      },
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown ¿Cuál es la finalidad de usar las funciones kernel en el modelo SVM?\n",
        "respuesta_1 = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FYTapl5LhTpz"
      },
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown ¿el valor de gamma afecta el kernel lineal? justificar teniendo cuenta la teoria y la documentación de la libreria.\n",
        "respuesta_2 = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X37FqhDQhTp0"
      },
      "source": [
        "Para analizar los resultados vamos a crear dos graficas para el mejor modelo encontrado:\n",
        "1. vamos a graficar en el eje x el valor real, en el eje y el valor predicho. El modelo ideal deberia ser una recta que recuerda la identidad\n",
        "2. en el eje x vamos a dejar un valor incremental y con colores vamos a diferenciar entre el valor real y el valor predicho\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S6KsCkihTp1"
      },
      "source": [
        "# dividir el conjunto\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#predicciones\n",
        "# OJO: Reemplazar los valores!\n",
        "Ypred =  predict_svr(X_train,y_train,X_test,kernel = ..., gamma =..., param_reg = ...)\n",
        "\n",
        "# plots\n",
        "\n",
        "f, ax = plt.subplots(ncols=2, sharex=False, sharey=False, figsize = (22,6))\n",
        "ax[0].scatter(y_test, Ypred)\n",
        "ax[0].set_xlabel('valor real', fontdict = {'fontsize': 12})\n",
        "ax[0].set_ylabel('valor predicho', fontdict = {'fontsize': 12})\n",
        "ax[1].plot(y_test, label = 'valor real', color = 'b', alpha = 0.7)\n",
        "ax[1].plot(Ypred, label = 'valor predicho', color = 'r', alpha = 0.5)\n",
        "ax[1].legend()\n",
        "ax[1].set_ylabel('Humedad relativa', fontdict = {'fontsize': 12})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0wploEQHhTp3"
      },
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown usando las anteriores graficas, ¿como calificaria el modelo de manera cualitativa?.\n",
        "respuesta_3 = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzJklaaBhTp5"
      },
      "source": [
        "## Ejercicio 3: Experimentar SVM para clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDd9cs8YhTp5"
      },
      "source": [
        "En este ejercicio vamos a volver a resolver el problema de calsificacion de dígitos. Vamos usar solo 4 clases y realizaremos un pre-procesamiento mediante PCA (una tecnica proxima a practicar en el laboratorio)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK18YvtnhTp5"
      },
      "source": [
        "Xcl, Ycl = load_digits(n_class=4,return_X_y=True)\n",
        "#--------- preprocesamiento--------------------\n",
        "pca = PCA(0.99, whiten=True)\n",
        "Xcl = pca.fit_transform(Xcl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9AsO_TrhTp7"
      },
      "source": [
        "Ahora vamos a crear la función para experimentar con la maquina de soporte vectorial. Para ellos vamos:\n",
        "\n",
        "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
        "2. Vamos a variar tres parámetros del SVC: kernel,  gamma y el parametro de regularización.\n",
        "3. Utilizar la metodología cross-validation con 4 folds más adecuada para problemas de clasificación.\n",
        "4. Usar normalización de datos estandar implementada por sklearn\n",
        "5. Extraer los vectores de soporte (observe los *atributos* del modelo SVC de sklearn). Recuerde que estos atributos son accesibles una vez el modelo es entrenado\n",
        "6. Utilizar como error el score de exactitud de la clasificación de sklearn.\n",
        "\n",
        "**Notas**: \n",
        "- Deberiamos poder acceder a las funciones de la libreria de sklearn directamente por el nombre sin necesidad de importarlas. Las funciones que deberios utilizar ya están precargadas en la configuración del laboratorio.\n",
        "- Llame todos los parametros de las funciones de sklearn de manera explicita. (i.e, si se quiere usar `max_iter` como parámetro para el SVC, debe crear el objeto: `SVC(max_iter = 100)`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHpENPahhTp7"
      },
      "source": [
        "#ejercicio de código\n",
        "def experiementarSVC(x, y, kernels, gammas,params_reg):\n",
        "    \"\"\"función que realizar experimentos sobre un SVM para clasificación\n",
        "    \n",
        "    x: numpy.Array, con las caracteristicas del problema\n",
        "    y: numpy.Array, con la variable objetivo\n",
        "    kernels: List[str], lista con valores a pasar \n",
        "        a sklearn correspondiente al kernel de la SVM\n",
        "    gammas: List[float], lista con los valores a pasar a\n",
        "        sklean correspondiente el valor de los coeficientes para usar en el\n",
        "        kernel\n",
        "    params_reg: List[float], lista con los valores a a pasar a \n",
        "        sklearn para ser usados como parametro de regularización\n",
        "    \n",
        "    retorna: pd.Dataframe con las siguientes columnas:\n",
        "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
        "        - error cuadratico medio en el cojunto entrenamiento (promedio de los 4 folds)\n",
        "        - error cuadratico medio en el cojunto test (promedio de los 4 folds)\n",
        "        - % de Vectores de Soporte promedio para los 4 folds (0 a 100)\n",
        "    \"\"\"\n",
        "    idx = 0\n",
        "    kf = ...(n_splits=4)\n",
        "    # crear una lista con la combinaciones de los elementos de cada list\n",
        "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
        "    resultados = pd.DataFrame()\n",
        "    \n",
        "    for params in kernels_gammas_regs:\n",
        "        kernel, gamma, param_reg = params\n",
        "        print(\"parametros usados\", params) # puede usar para ver los params\n",
        "        errores_train = []\n",
        "        errores_test = []\n",
        "        pct_support_vectors = []\n",
        "        for train_index, test_index in kf.split(...):\n",
        "            X_train, X_test = x[train_index], x[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]  \n",
        "            # normalizar los datos\n",
        "            scaler = ...\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "            svm = ...\n",
        "            # Entrenar el modelo\n",
        "            svm....(X=..., y=...)\n",
        "            # calculo de errores\n",
        "            y_train_pred = svm...(X=...)\n",
        "            y_test_pred = svm...(X=...)\n",
        "            # error y pct de vectores de soporte\n",
        "            errores_train.append(...(y_true = y_train, y_pred = y_train_pred))\n",
        "            errores_test.append(...(y_true = y_test, y_pred = y_test_pred))\n",
        "            # contar muestras de entrenamiento\n",
        "            n_train = X_train.shape[0]\n",
        "            pct_vs = (.../...)\n",
        "            pct_support_vectors.append(pct_vs)\n",
        "        \n",
        "        resultados.loc[idx,'kernel'] = kernel\n",
        "        resultados.loc[idx,'gamma'] = gamma\n",
        "        resultados.loc[idx,'param_reg'] = param_reg\n",
        "        resultados.loc[idx,'error de entrenamiento'] = np...(errores_train)\n",
        "        resultados.loc[idx,'error de prueba'] = np...(errores_test)\n",
        "        resultados.loc[idx,'% de vectores de soporte'] = np...(pct_support_vectors)\n",
        "        idx+=1\n",
        "    return (resultados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPY9emZGhTp-"
      },
      "source": [
        "GRADER.run_test(\"ejercicio3\", experiementarSVC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLwkxs1fhTp_"
      },
      "source": [
        "# vamos a realizar los experimentos\n",
        "resultadosSVC = experiementarSVC(x = Xcl,y=Ycl,\n",
        "                                 kernels=['linear', 'rbf'],\n",
        "                                 gammas = [0.01,0.1],\n",
        "                                 params_reg = [0.001, 0.01,0.1, 1.0,10]\n",
        "                                )\n",
        "\n",
        "resultadosSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YcGKqkW8hTqB"
      },
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown Explique en sus palabras ¿qué representan los vectores de soporte en un problema de clasificación? ¿Esa misma explicación se puede aplicar a un problema de regresión? justifique\n",
        "respuesta_4 = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "737HUyUqhTqD"
      },
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown Observando la documentación de SVC, sklearn usa estrategia one-vs-one. ¿Que diferencia existe al aplicar una estrategia  one-vs-rest?\n",
        "respuesta_5 = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta2szRQRhTqE"
      },
      "source": [
        "# ver la relación de parametro de regularización y los vectores de soporte\n",
        "import seaborn as sns\n",
        "ax= sns.relplot(data = resultadosSVC, x = 'param_reg', y = '% de vectores de soporte', kind = 'line', hue ='kernel', aspect = 1.5)\n",
        "ax.set(xscale=\"log\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pPGG4r9ShTqG"
      },
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown ¿qué relación observa entre el valor del parametro de regularización y los vectores de soporte? ¿como explica esta relación?\n",
        "respuesta_6 = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1WJpOijhTqH"
      },
      "source": [
        "**Para Tener en cuenta** \n",
        "\n",
        "sklearn tiene unos \"wrappers\", que implementan estrategias para la clasificación multiclase, uno  de estos wrappers, implementa la estrategia one-vs-rest: https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html.\n",
        "\n",
        "Un wrapper, es un esquema de diseño común para \"envolver\" librerias/funciones con caracteristicas similares y poder modificar ciertos comportamientos. Vale la pena observar los ejemplos para saber como utilizar estas estrategias cuando sea necesario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnTq9RTehTqH"
      },
      "source": [
        "GRADER.check_tests()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dQ-WZCK-hTqJ"
      },
      "source": [
        "#@title Integrantes\n",
        "codigo_integrante_1 ='' #@param {type:\"string\"}\n",
        "codigo_integrante_2 = ''  #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va7hyAs6hTqK"
      },
      "source": [
        "----\n",
        "esta linea de codigo va fallar, es de uso exclusivo de los profesores\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek0GsP_0hTqL"
      },
      "source": [
        "GRADER.grade()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}