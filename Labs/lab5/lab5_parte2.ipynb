{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7h9z6smhTpe"
      },
      "source": [
        "**Recuerda que una vez abierto, Da clic en \"Copiar en Drive\", de lo contrario no podras almacenar tu progreso**\n",
        "\n",
        "Nota: no olvide ir ejecutando las celdas de código de arriba hacia abajo para que no tenga errores de importación de librerías o por falta de definición de variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_VhwZwtm9I5"
      },
      "outputs": [],
      "source": [
        "#configuración del laboratorio\n",
        "# Ejecuta esta celda!\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "in_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYer0hoGm9I6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not in_colab:\n",
        "    import sys ; sys.path.append('../commons/utils/'); sys.path.append('../commons/utils/data')\n",
        "else: \n",
        "    os.system('wget https://raw.githubusercontent.com/jdariasl/ML_2020/master/Labs/commons/utils/general.py -O general.py')\n",
        "    from general import configure_lab5_2\n",
        "    configure_lab5_2()\n",
        "from lab5 import *\n",
        "GRADER, dataset = part_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59PRMgRphTpf"
      },
      "source": [
        "# Laboratorio 5 - Parte 2. Máquinas de Vectores de Soporte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbFfmXl-hTpi"
      },
      "source": [
        "### Ejercicio 1: Limipiar base de datos y completar código\n",
        "\n",
        "En este ejercicio usaremos la regresión por vectores de soporte para resolver el problema de regresión de la base de datos AirQuality (https://archive.ics.uci.edu/ml/datasets/Air+Quality). Tener en cuenta que vamos a usar solo 2000 muestras.\n",
        "\n",
        "En primera instancia vamos a transformar la matriz en un dataframe, para poderlo procesar de manera mas sencilla. Se crea una columna por cada variable que obtenemos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpuAUEFBm9I7"
      },
      "outputs": [],
      "source": [
        "dataset_df = pd.DataFrame(dataset, columns = [f'col_{c}' for c in range (1,14)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPSDZfHIm9I8"
      },
      "source": [
        "Para esta base de datos vamos a realizar una limpieza de datos. \n",
        "Para ello vamos a completar la siguiente función para realizar:\n",
        "    \n",
        "1. **Remover** todos registros cuya variable objetivo es faltante (missing Value). Estos registros están marcados como -200, es decir, donde haya un valor -200 eliminaremos el registro.\n",
        "2. **imputar los valores perdidos/faltantes** en cada una de las características, vamos a usar el valor medio de la característica en especifico.\n",
        "3. **Verificar** si quedaron valores faltantes\n",
        "4. **retornar**X (12 primeras columnas) y Y(la 13 columna).\n",
        "\n",
        "informacion de utilidad:\n",
        "\n",
        "1. Aca puede ser de utilidad recordar nuestra [sesión extra](https://jdariasl.github.io/ML_2020/Labs/Extra/Basic_Preprocessing_FeatureEngineering.html).\n",
        "2. Para transformar columnas de pandas a arreglos de numpy se puede usar `.iloc` / `.loc` y . `.values`, por ejemplo  para devolver una matriz con los valores de las primeras dos columnas es posible hacerlo asi: `dataset_df.iloc[: , 0:2].values` o `dataset_df.loc[: , ['col_1', 'col_2']].values`\n",
        "3. Para cambiar valores faltantes, podemos usar la [librería sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBT8aPiihTpi"
      },
      "outputs": [],
      "source": [
        "# ejercicio de codigo\n",
        "def clean_data(df):\n",
        "    \"\"\"funcion que limpia el dataset y obtiene X y Y\n",
        "    \n",
        "    df: es un pandas dataframe\n",
        "    \n",
        "    retorna:\n",
        "    X: una matriz numpy con los valores a usar como conjunto de datos\n",
        "       de entrada\n",
        "    Y una matriz numpy con los valores usados como conjunto de datos\n",
        "       de salida\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # se copia el df para evitar cambios sobre el objeto\n",
        "    database = df.copy()\n",
        "    \n",
        "    ##Verificar\n",
        "    pct_valores_faltantes = (database==-200).mean()\n",
        "    print(\",\".join([f\"% VF en {a}: {pct_valores_faltantes[a]*100:.2f}% \" for a in pct_valores_faltantes.index]))\n",
        "    \n",
        "    # identificar muetras cuya salida en un valor faltante\n",
        "    idx_to_remove = []\n",
        "    for idx in database.index:\n",
        "        ## reemplazar el valor\n",
        "        if database.iloc[idx,...] == ...:\n",
        "            idx_to_remove.append(idx)\n",
        "    \n",
        "    #remover la muestras de los indices\n",
        "    database = database.drop(idx_to_remove,axis = 0)\n",
        "\n",
        "    print (\"\\nHay \" + str(len(idx_to_remove)) + \" valores perdidos en la variable de salida.\")\n",
        "    print (\"\\nDim de la base de datos sin las muestras con variable de salida perdido \"+ str(np.shape(database)))\n",
        "\n",
        "    ##Imputar\n",
        "    print (\"\\nProcesando imputación de valores perdidos en las características . . .\")\n",
        "    imputer = ... (missing_values= ... , strategy= ...)\n",
        "    \"imputar solo las columnas de las variables de entrada\"\n",
        "    database.iloc[:,0:...] = imputer.fit_transform(database.iloc[:,...] )\n",
        "\n",
        "    print (\"Imputación finalizada.\\n\")\n",
        "\n",
        "    ##Verificar\n",
        "    pct_valores_faltantes = (database==-200).mean()\n",
        "    \n",
        "    print(\",\".join([f\"% VF en {a}: {pct_valores_faltantes[a]*100:.2f}% \" for a in pct_valores_faltantes.index]))\n",
        "    \n",
        "    if(pct_valores_faltantes.max() != 0):\n",
        "        print (\"Hay valores perdidos\")\n",
        "    else:\n",
        "        print (\"No hay valores perdidos en la base de datos. Ahora se puede procesar\")\n",
        "\n",
        "    X = database.iloc[:,0:12].values\n",
        "    Y = database.iloc[:,12:13].values\n",
        "    return (X,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_aHK7XThTpk"
      },
      "outputs": [],
      "source": [
        "# ignorar los prints\n",
        "GRADER.run_test(\"ejercicio1\", clean_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_siuGCthTpm"
      },
      "source": [
        "Ahora usemos la función para tener nuestras variables X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJLs3jUohTpm"
      },
      "outputs": [],
      "source": [
        "X,Y = clean_data(dataset_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfT-7D5AhTpo"
      },
      "source": [
        "### Ejercicio 2: Experimentar SVM para regresión\n",
        "\n",
        "Ahora vamos a crear la función para experimentar con la maquina de soporte vectorial. Para ellos vamos:\n",
        "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html. \n",
        "2. Vamos a variar tres parámetros del SVR: kernel,  gamma y el parametro de regularización.\n",
        "3. Utilizar la metodología cross-validation con 4 folds.\n",
        "4. Usar normalización de datos estandar implementada por sklearn\n",
        "5. Extraer los vectores de soporte (observe los *atributos* del modelo SVR de sklearn). Recuerde que estos atributos son accesibles, una vez el modelo es entrenado\n",
        "6. Utilizar el metrica para calcular el MAPE (usar sklearn).\n",
        "\n",
        "**Notas**: \n",
        "- Deberiamos poder acceder a las funciones de la libreria de sklearn directamente por el nombre sin necesidad de importarlas. Las funciones que deberios utilizar ya están precargadas en la configuración del laboratorio.\n",
        "- Llame todos los parametros de las funciones de sklearn de manera explicita. (i.e, si se quiere usar `max_iter` como parámetro para el SVR, debe crear el objeto: `SVR(max_iter = 100)`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84KNyMMuhTpp"
      },
      "outputs": [],
      "source": [
        "#ejercicio de código\n",
        "def experiementarSVR(x, y, kernels, gammas,params_reg):\n",
        "    \"\"\"función que realizar experimentos sobre un SVM para regresión\n",
        "    \n",
        "    x: numpy.Array, con las caracteristicas del problema\n",
        "    y: numpy.Array, con la variable objetivo\n",
        "    kernels: List[str], lista con valores a pasar \n",
        "        a sklearn correspondiente al kernel de la SVM\n",
        "    gammas: List[float], lista con los valores a pasar a\n",
        "        sklean correspondiente el valor de los coeficientes para usar en el\n",
        "        kernel\n",
        "    params_reg: List[float], lista con los valores a a pasar a \n",
        "        sklearn para ser usados como parametro de regularización\n",
        "    \n",
        "    retorna: pd.Dataframe con las siguientes columnas:\n",
        "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
        "        - error cuadratico medio en el cojunto test (promedio de los 5 folds)\n",
        "        - intervalo de confianza del error cuadratico medio en el cojunto test \n",
        "            (desviacion estandar de los 5 folds)\n",
        "        - # de Vectores de Soporte promedio para los 5 folds\n",
        "        - % de Vectores de Soporte promedio para los 5 folds (0 a 100)\n",
        "    \"\"\"\n",
        "    idx = 0\n",
        "    kf = ...(n_splits=...)\n",
        "    # crear una lista con la combinaciones de los elementos de cada list\n",
        "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
        "    resultados = pd.DataFrame()\n",
        "    \n",
        "    for params in kernels_gammas_regs:\n",
        "        kernel, gamma, param_reg = params\n",
        "        print(\"parametros usados\", params) # puede usar para ver los params\n",
        "        errores_test = []\n",
        "        pct_support_vectors = []\n",
        "        num_support_vectors = []\n",
        "        for train_index, test_index in kf...(...)\n",
        "            \n",
        "            X_train, X_test = x[train_index], x[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]  \n",
        "            # normalizar los datos\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "            svm = ...(kernel = ...., gamma = ..., C = ...)\n",
        "            # Entrenar el modelo\n",
        "            svm.fit(X=X_train, y=...)\n",
        "            # Validación del modelo\n",
        "            ypred = svm.predict(X=...)\n",
        "            \n",
        "            # error y pct de vectores de soporte\n",
        "            errores_test.append(...(y_true = y_test, y_pred = ypred))\n",
        "            # contar muestras de entrenamiento\n",
        "            n_train = X_train.shape[0]\n",
        "            num_vs = len(svm...)\n",
        "            pct_vs = (... / n_train ) *100\n",
        "            pct_support_vectors.append(pct_vs)\n",
        "            num_support_vectors.append(num_vs)\n",
        "        \n",
        "        resultados.loc[idx,'kernel'] = kernel\n",
        "        resultados.loc[idx,'gamma'] = gamma\n",
        "        resultados.loc[idx,'param_reg'] = param_reg\n",
        "        resultados.loc[idx,'error de prueba (promedio)'] = np.mean(...)\n",
        "        resultados.loc[idx,'error de prueba (intervalo de confianza)'] = np.std(errores_test)\n",
        "        resultados.loc[idx,'# de vectores de soporte'] = np.mean(...)\n",
        "        resultados.loc[idx,'% de vectores de soporte'] = np.mean(...)\n",
        "        \n",
        "        idx+=1\n",
        "    return (resultados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB4bMhUhhTpr"
      },
      "outputs": [],
      "source": [
        "GRADER.run_test(\"ejercicio2\", experiementarSVR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aatsj0EQhTpt"
      },
      "source": [
        "Para entrenar vamos a ignorar las dos primeras variables, estas corresponden a valores de fechas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRHwQhR7hTpu"
      },
      "outputs": [],
      "source": [
        "# vamos a realizar los experimentos\n",
        "resultadosSVR = experiementarSVR(x =X[:,2:],y=Y,\n",
        "                                 kernels=['linear', 'rbf'],\n",
        "                                 gammas = [0.01,0.1],\n",
        "                                 params_reg = [0.1, 1.0,10]\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEPmKLZhhTpw"
      },
      "outputs": [],
      "source": [
        "# ver los 5 primeros resultados\n",
        "resultadosSVR.sort_values('error de prueba (promedio)',ascending=True).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qERAELnQhTpx"
      },
      "outputs": [],
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown ¿Cual es el objetivo de las las funciones kernel? Contestar dentro del contexto de las máquinas de sporte vectorial\n",
        "respuesta_1 = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wCsV08SCm9JB"
      },
      "outputs": [],
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown Explique en sus palabras ¿qué representan los vectores de soporte en un problema de regresión?\n",
        "respuesta_2 = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X37FqhDQhTp0"
      },
      "source": [
        "Para analizar los resultados vamos a crear dos graficas para el mejor modelo encontrado:\n",
        "1. vamos a graficar en el eje x el valor real, en el eje y el valor predicho. El modelo ideal deberia ser una recta que recuerda la identidad\n",
        "2. en el eje x vamos a dejar un valor incremental y con colores vamos a diferenciar entre el valor real y el valor predicho\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S6KsCkihTp1"
      },
      "outputs": [],
      "source": [
        "# dividir el conjunto\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#predicciones\n",
        "# OJO: Reemplazar los valores!\n",
        "Ypred =  predict_svr(X_train,y_train,X_test,kernel = ..., )\n",
        "\n",
        "# plots\n",
        "\n",
        "f, ax = plt.subplots(ncols=2, sharex=False, sharey=False, figsize = (22,6))\n",
        "ax[0].scatter(y_test, Ypred)\n",
        "ax[0].set_xlabel('valor real', fontdict = {'fontsize': 12})\n",
        "ax[0].set_ylabel('valor predicho', fontdict = {'fontsize': 12})\n",
        "ax[1].plot(y_test, label = 'valor real', color = 'b', alpha = 0.7)\n",
        "ax[1].plot(Ypred, label = 'valor predicho', color = 'r', alpha = 0.5)\n",
        "ax[1].legend()\n",
        "ax[1].set_ylabel('Humedad relativa', fontdict = {'fontsize': 12})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0wploEQHhTp3"
      },
      "outputs": [],
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown usando las anteriores graficas, ¿como calificaria el modelo de manera cualitativa?.\n",
        "respuesta_3 = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzJklaaBhTp5"
      },
      "source": [
        "### Ejercicio 3: Experimentar SVM para clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDd9cs8YhTp5"
      },
      "source": [
        "En este ejercicio vamos a volver a resolver el problema de clasificación de dígitos. Vamos usar solo 5 clases y realizaremos un pre-procesamiento:\n",
        "1. mediante PCA (una tecnica proxima a practicar en el laboratorio)\n",
        "2. Vamos a convertirlo en problema biclase (vamos a diferenciar entre 0, 1 y el resto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK18YvtnhTp5"
      },
      "outputs": [],
      "source": [
        "Xcl, Ycl = load_digits(n_class=5,return_X_y=True)\n",
        "#--------- preprocesamiento--------------------\n",
        "pca = PCA(0.99, whiten=True)\n",
        "Xcl = pca.fit_transform(Xcl)\n",
        "# cambiar problema de clases\n",
        "unique, counts  = np.unique(Ycl, return_counts=True)\n",
        "print(\"distribución original (claves las etiquetas, valores el número de muestras): \\n\", dict(zip(unique, counts )))\n",
        "Ycl = np.where(np.isin(Ycl, [0,1]), Ycl, 2)\n",
        "unique, counts  = np.unique(Ycl, return_counts=True)\n",
        "print(\"Nueva distribución  (claves las etiquetas, valores el número de muestras): \\n\", dict(zip(unique, counts )))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9AsO_TrhTp7"
      },
      "source": [
        "Ahora vamos a crear la función para experimentar con la maquina de soporte vectorial. Para ellos vamos:\n",
        "\n",
        "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
        "2. Vamos a variar tres parámetros del SVC: kernel,  gamma y el parametro de regularización.\n",
        "3. Utilizar la metodología cross-validation con 4 folds más adecuada para problemas de clasificación.\n",
        "4. Usar normalización de datos estandar implementada por sklearn\n",
        "5. Extraer los vectores de soporte (observe los *atributos* del modelo SVC de sklearn). Recuerde que estos atributos son accesibles una vez el modelo es entrenado\n",
        "6. vamos a probar la dos estragegias del SVC:\n",
        "    - One Vs One\n",
        "    - One Vs Rest\n",
        "7. Utilizar como error el score de exactitud de la clasificación de sklearn.\n",
        "\n",
        "**Notas**: \n",
        "- Deberiamos poder acceder a las funciones de la libreria de sklearn directamente por el nombre sin necesidad de importarlas. Las funciones que deberios utilizar ya están precargadas en la configuración del laboratorio.\n",
        "- Llame todos los parametros de las funciones de sklearn de manera explicita. (i.e, si se quiere usar `max_iter` como parámetro para el SVC, debe crear el objeto: `SVC(max_iter = 100)`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggnW0A7hm9JD"
      },
      "source": [
        "* sklearn tiene unos \"wrappers\", que implementan estrategias para la clasificación multiclase, uno  de estos wrappers, implementa la estrategia one-vs-rest: https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html.\n",
        "\n",
        "* Un wrapper, es un esquema de diseño común para \"envolver\" librerias/funciones con caracteristicas similares y poder modificar ciertos comportamientos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHpENPahhTp7"
      },
      "outputs": [],
      "source": [
        "#ejercicio de código\n",
        "def experiementarSVC(x, y, kernels, gammas,params_reg, estrategia = 'ovo'):\n",
        "    \"\"\"función que realizar experimentos sobre un SVM para clasificación\n",
        "    \n",
        "    x: numpy.Array, con las caracteristicas del problema\n",
        "    y: numpy.Array, con la variable objetivo\n",
        "    kernels: List[str], lista con valores a pasar \n",
        "        a sklearn correspondiente al kernel de la SVM\n",
        "    gammas: List[float], lista con los valores a pasar a\n",
        "        sklean correspondiente el valor de los coeficientes para usar en el\n",
        "        kernel\n",
        "    params_reg: List[float], lista con los valores a a pasar a \n",
        "        sklearn para ser usados como parametro de regularización\n",
        "    estrategia: str, valor que puede ser ovo (para one vs one) o ovr \n",
        "        (para one vs rest)\n",
        "    \n",
        "    retorna: pd.Dataframe con las siguientes columnas:\n",
        "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
        "        - error cuadratico medio en el cojunto entrenamiento (promedio de los 4 folds)\n",
        "        - error cuadratico medio en el cojunto test (promedio de los 4 folds)\n",
        "        - % de Vectores de Soporte promedio para los 4 folds (0 a 100)\n",
        "    \"\"\"\n",
        "    idx = 0\n",
        "    kf = ...(n_splits=...)\n",
        "    # crear una lista con la combinaciones de los elementos de cada list\n",
        "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
        "    resultados = pd.DataFrame()\n",
        "    \n",
        "    for params in kernels_gammas_regs:\n",
        "        kernel, gamma, param_reg = params\n",
        "        print(\"parametros usados\", params) # puede usar para ver los params\n",
        "        errores_train = []\n",
        "        errores_test = []\n",
        "        pct_support_vectors = []\n",
        "        \n",
        "        for train_index, test_index in kf...(...,...):\n",
        "            X_train, X_test = x[train_index], x[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]  \n",
        "            # normalizar los datos\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "            \n",
        "            svm = SVC(kernel = ..., gamma = ..., C = ...)\n",
        "            \n",
        "            # si es estrategia \"envolver\" a la svm\n",
        "            if estrategia =='ovr':\n",
        "                svm = ... (svm)               \n",
        "            \n",
        "            # Entrenar el modelo\n",
        "            svm...(X=X_train, y=y_train)\n",
        "            # calculo de errores\n",
        "            y_train_pred = svm...(X=X_train)\n",
        "            y_test_pred = svm...(X=X_test)\n",
        "            # error y pct de vectores de soporte\n",
        "            errores_train.append(accuracy_score(y_true = y_train, y_pred = y_train_pred))\n",
        "            errores_test.append(accuracy_score(y_true = y_test, y_pred = y_test_pred))\n",
        "            # contar muestras de entrenamiento\n",
        "            n_train = X_train.shape[0]\n",
        "            if estrategia == 'ovr':\n",
        "                # en esta estrategia se realizar una SVM por cada clase\n",
        "                # por lo tanto tenemos que acceder a cada una de la SVM\n",
        "                # lee la documentacion\n",
        "                num_vs = np.mean([len(svc...) for svc in svm....])\n",
        "                pct_vs = (num_vs/n_train)*100\n",
        "                \n",
        "            else:\n",
        "                # cuando es ovo solo tenemos una SVM\n",
        "                pct_vs = (len(svm...)/n_train)*100\n",
        "            pct_support_vectors.append(pct_vs)\n",
        "        \n",
        "        resultados.loc[idx,'kernel'] = kernel\n",
        "        resultados.loc[idx,'gamma'] = gamma\n",
        "        resultados.loc[idx,'param_reg'] = param_reg\n",
        "        resultados.loc[idx,'estrategia'] = ...\n",
        "        resultados.loc[idx,'error de entrenamiento'] = np.mean(errores_train)\n",
        "        resultados.loc[idx,'error de prueba'] = np.mean(errores_test)\n",
        "        resultados.loc[idx,'% de vectores de soporte'] = np.mean(...)\n",
        "        idx+=1\n",
        "    return (resultados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPY9emZGhTp-"
      },
      "outputs": [],
      "source": [
        "GRADER.run_test(\"ejercicio3\", experiementarSVC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtuL_Gunm9JE"
      },
      "source": [
        "Veamos la estrategia OVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLwkxs1fhTp_"
      },
      "outputs": [],
      "source": [
        "# vamos a realizar los experimentos\n",
        "resultadosSVC_ovr = experiementarSVC(x = Xcl,y=Ycl,\n",
        "                                 kernels=['linear', 'rbf'],\n",
        "                                 gammas = [0.01,0.1],\n",
        "                                 params_reg = [0.001, 0.01,0.1, 1.0,10],\n",
        "                                estrategia = 'ovr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpbX_1N2m9JE"
      },
      "outputs": [],
      "source": [
        "# ver los mejores modelos\n",
        "resultadosSVC_ovr.sort_values('error de prueba', ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMLzgKVhm9JE"
      },
      "source": [
        "Ahora vamos a ver la estrategia OVO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BPy6leLm9JE"
      },
      "outputs": [],
      "source": [
        "# vamos a realizar los experimentos\n",
        "resultadosSVC_ovo = experiementarSVC(x = Xcl,y=Ycl,\n",
        "                                 kernels=['linear', 'rbf'],\n",
        "                                 gammas = [0.01,0.1],\n",
        "                                 params_reg = [0.001, 0.01,0.1, 1.0,10],\n",
        "                                estrategia = 'ovo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eWC6Xj4m9JE"
      },
      "outputs": [],
      "source": [
        "# ver los mejores modelos\n",
        "resultadosSVC_ovo.sort_values('error de prueba', ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YcGKqkW8hTqB"
      },
      "outputs": [],
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown Explique en sus palabras ¿qué representan los vectores de soporte en un problema de clasificación?\n",
        "respuesta_4 = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "737HUyUqhTqD"
      },
      "outputs": [],
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown Según el tipo de problema (enfocarse en la distribución de clases) ¿La métrica usada es la adecuada? ¿Cual otra métrica del modulo de sklearn podría ser usada?\n",
        "respuesta_5 = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta2szRQRhTqE"
      },
      "outputs": [],
      "source": [
        "# ver la relación de parametro de regularización y los vectores de soporte\n",
        "import seaborn as sns\n",
        "ax= sns.relplot(data = resultadosSVC_ovo, x = 'param_reg', y = '% de vectores de soporte', kind = 'line', hue ='kernel', aspect = 1.5)\n",
        "ax.set(xscale=\"log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pPGG4r9ShTqG"
      },
      "outputs": [],
      "source": [
        "#@title Pregunta Abierta\n",
        "#@markdown ¿qué relación observa entre el valor del parametro de regularización y los vectores de soporte? ¿como explica esta relación?\n",
        "respuesta_6 = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnTq9RTehTqH"
      },
      "outputs": [],
      "source": [
        "GRADER.check_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dQ-WZCK-hTqJ"
      },
      "outputs": [],
      "source": [
        "#@title Integrantes\n",
        "codigo_integrante_1 ='' #@param {type:\"string\"}\n",
        "codigo_integrante_2 = ''  #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va7hyAs6hTqK"
      },
      "source": [
        "----\n",
        "esta linea de codigo va fallar, es de uso exclusivo de los profesores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek0GsP_0hTqL"
      },
      "outputs": [],
      "source": [
        "GRADER.grade()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab5_parte2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}