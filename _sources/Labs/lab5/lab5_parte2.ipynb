{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7h9z6smhTpe"
   },
   "source": [
    "**Recuerda que una vez abierto, Da clic en \"Copiar en Drive\", de lo contrario no podras almacenar tu progreso**\n",
    "\n",
    "Nota: no olvide ir ejecutando las celdas de código de arriba hacia abajo para que no tenga errores de importación de librerías o por falta de definición de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuración del laboratorio\n",
    "# Ejecuta esta celda!\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "in_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not in_colab:\n",
    "    import sys ; sys.path.append('../commons/utils/'); sys.path.append('../commons/utils/data')\n",
    "else: \n",
    "    os.system('wget https://raw.githubusercontent.com/jdariasl/ML_2020/master/Labs/commons/utils/general.py -O general.py')\n",
    "    from general import configure_lab5_2\n",
    "    configure_lab5_2()\n",
    "from lab5 import *\n",
    "GRADER, dataset = part_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59PRMgRphTpf"
   },
   "source": [
    "# Laboratorio 5 - Parte 2. Máquinas de Vectores de Soporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbFfmXl-hTpi"
   },
   "source": [
    "### Ejercicio 1: Limipiar base de datos y completar código\n",
    "\n",
    "En este ejercicio usaremos la regresión por vectores de soporte para resolver el problema de regresión de la base de datos AirQuality (https://archive.ics.uci.edu/ml/datasets/Air+Quality). Tener en cuenta que vamos a usar solo 2000 muestras.\n",
    "\n",
    "En primera instancia vamos a transformar la matriz en un dataframe, para poderlo procesar de manera mas sencilla. Se crea una columna por cada variable que obtenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(dataset, columns = [f'col_{c}' for c in range (1,14)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta base de datos vamos a realizar una limpieza de datos. \n",
    "Para ello vamos a completar la siguiente función para realizar:\n",
    "    \n",
    "1. **Remover** todos registros cuya variable objetivo es faltante (missing Value). Estos registros están marcados como -200, es decir, donde haya un valor -200 eliminaremos el registro.\n",
    "2. **imputar los valores perdidos/faltantes** en cada una de las características, vamos a usar el valor medio de la característica en especifico.\n",
    "3. **Verificar** si quedaron valores faltantes\n",
    "4. **retornar**X (12 primeras columnas) y Y(la 13 columna).\n",
    "\n",
    "informacion de utilidad:\n",
    "\n",
    "1. Aca puede ser de utilidad recordar nuestra [sesión extra](https://jdariasl.github.io/ML_2020/Labs/Extra/Basic_Preprocessing_FeatureEngineering.html).\n",
    "2. Para transformar columnas de pandas a arreglos de numpy se puede usar `.iloc` / `.loc` y . `.values`, por ejemplo  para devolver una matriz con los valores de las primeras dos columnas es posible hacerlo asi: `dataset_df.iloc[: , 0:2].values` o `dataset_df.loc[: , ['col_1', 'col_2']].values`\n",
    "3. Para cambiar valores faltantes, podemos usar la [librería sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBT8aPiihTpi"
   },
   "outputs": [],
   "source": [
    "# ejercicio de codigo\n",
    "def clean_data(df):\n",
    "    \"\"\"funcion que limpia el dataset y obtiene X y Y\n",
    "    \n",
    "    df: es un pandas dataframe\n",
    "    \n",
    "    retorna:\n",
    "    X: una matriz numpy con los valores a usar como conjunto de datos\n",
    "       de entrada\n",
    "    Y una matriz numpy con los valores usados como conjunto de datos\n",
    "       de salida\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # se copia el df para evitar cambios sobre el objeto\n",
    "    database = df.copy()\n",
    "    \n",
    "    ##Verificar\n",
    "    pct_valores_faltantes = (database==-200).mean()\n",
    "    print(\",\".join([f\"% VF en {a}: {pct_valores_faltantes[a]*100:.2f}% \" for a in pct_valores_faltantes.index]))\n",
    "    \n",
    "    # identificar muetras cuya salida en un valor faltante\n",
    "    idx_to_remove = []\n",
    "    for idx in database.index:\n",
    "        ## reemplazar el valor\n",
    "        if database.iloc[idx,...] == ...:\n",
    "            idx_to_remove.append(idx)\n",
    "    \n",
    "    #remover la muestras de los indices\n",
    "    database = database.drop(idx_to_remove,axis = 0)\n",
    "\n",
    "    print (\"\\nHay \" + str(len(idx_to_remove)) + \" valores perdidos en la variable de salida.\")\n",
    "    print (\"\\nDim de la base de datos sin las muestras con variable de salida perdido \"+ str(np.shape(database)))\n",
    "\n",
    "    ##Imputar\n",
    "    print (\"\\nProcesando imputación de valores perdidos en las características . . .\")\n",
    "    imputer = ... (missing_values= ... , strategy= ...)\n",
    "    \"imputar solo las columnas de las variables de entrada\"\n",
    "    database.iloc[:,0:...] = imputer.fit_transform(database.iloc[:,...] )\n",
    "\n",
    "    print (\"Imputación finalizada.\\n\")\n",
    "\n",
    "    ##Verificar\n",
    "    pct_valores_faltantes = (database==-200).mean()\n",
    "    \n",
    "    print(\",\".join([f\"% VF en {a}: {pct_valores_faltantes[a]*100:.2f}% \" for a in pct_valores_faltantes.index]))\n",
    "    \n",
    "    if(pct_valores_faltantes.max() != 0):\n",
    "        print (\"Hay valores perdidos\")\n",
    "    else:\n",
    "        print (\"No hay valores perdidos en la base de datos. Ahora se puede procesar\")\n",
    "\n",
    "    X = database.iloc[:,0:12].values\n",
    "    Y = database.iloc[:,12:13].values\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_aHK7XThTpk"
   },
   "outputs": [],
   "source": [
    "# ignorar los prints\n",
    "GRADER.run_test(\"ejercicio1\", clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_siuGCthTpm"
   },
   "source": [
    "Ahora usemos la función para tener nuestras variables X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJLs3jUohTpm"
   },
   "outputs": [],
   "source": [
    "X,Y = clean_data(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfT-7D5AhTpo"
   },
   "source": [
    "### Ejercicio 2: Experimentar SVM para regresión\n",
    "\n",
    "Ahora vamos a crear la función para experimentar con la maquina de soporte vectorial. Para ellos vamos:\n",
    "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html. \n",
    "2. Vamos a variar tres parámetros del SVR: kernel,  gamma y el parametro de regularización.\n",
    "3. Utilizar la metodología cross-validation con 4 folds.\n",
    "4. Usar normalización de datos estandar implementada por sklearn\n",
    "5. Extraer los vectores de soporte (observe los *atributos* del modelo SVR de sklearn). Recuerde que estos atributos son accesibles, una vez el modelo es entrenado\n",
    "6. Utilizar el metrica para calcular el MAPE (usar sklearn).\n",
    "\n",
    "**Notas**: \n",
    "- Deberiamos poder acceder a las funciones de la libreria de sklearn directamente por el nombre sin necesidad de importarlas. Las funciones que deberios utilizar ya están precargadas en la configuración del laboratorio.\n",
    "- Llame todos los parametros de las funciones de sklearn de manera explicita. (i.e, si se quiere usar `max_iter` como parámetro para el SVR, debe crear el objeto: `SVR(max_iter = 100)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84KNyMMuhTpp"
   },
   "outputs": [],
   "source": [
    "#ejercicio de código\n",
    "def experiementarSVR(x, y, kernels, gammas,params_reg):\n",
    "    \"\"\"función que realizar experimentos sobre un SVM para regresión\n",
    "    \n",
    "    x: numpy.Array, con las caracteristicas del problema\n",
    "    y: numpy.Array, con la variable objetivo\n",
    "    kernels: List[str], lista con valores a pasar \n",
    "        a sklearn correspondiente al kernel de la SVM\n",
    "    gammas: List[float], lista con los valores a pasar a\n",
    "        sklean correspondiente el valor de los coeficientes para usar en el\n",
    "        kernel\n",
    "    params_reg: List[float], lista con los valores a a pasar a \n",
    "        sklearn para ser usados como parametro de regularización\n",
    "    \n",
    "    retorna: pd.Dataframe con las siguientes columnas:\n",
    "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
    "        - error cuadratico medio en el cojunto test (promedio de los 5 folds)\n",
    "        - intervalo de confianza del error cuadratico medio en el cojunto test \n",
    "            (desviacion estandar de los 5 folds)\n",
    "        - # de Vectores de Soporte promedio para los 5 folds\n",
    "        - % de Vectores de Soporte promedio para los 5 folds (0 a 100)\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    kf = ...(n_splits=...)\n",
    "    # crear una lista con la combinaciones de los elementos de cada list\n",
    "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
    "    resultados = pd.DataFrame()\n",
    "    \n",
    "    for params in kernels_gammas_regs:\n",
    "        kernel, gamma, param_reg = params\n",
    "        print(\"parametros usados\", params) # puede usar para ver los params\n",
    "        errores_test = []\n",
    "        pct_support_vectors = []\n",
    "        num_support_vectors = []\n",
    "        for train_index, test_index in kf...(...)\n",
    "            \n",
    "            X_train, X_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]  \n",
    "            # normalizar los datos\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            svm = ...(kernel = ...., gamma = ..., C = ...)\n",
    "            # Entrenar el modelo\n",
    "            svm.fit(X=X_train, y=...)\n",
    "            # Validación del modelo\n",
    "            ypred = svm.predict(X=...)\n",
    "            \n",
    "            # error y pct de vectores de soporte\n",
    "            errores_test.append(...(y_true = y_test, y_pred = ypred))\n",
    "            # contar muestras de entrenamiento\n",
    "            n_train = X_train.shape[0]\n",
    "            num_vs = len(svm...)\n",
    "            pct_vs = (... / n_train ) *100\n",
    "            pct_support_vectors.append(pct_vs)\n",
    "            num_support_vectors.append(num_vs)\n",
    "        \n",
    "        resultados.loc[idx,'kernel'] = kernel\n",
    "        resultados.loc[idx,'gamma'] = gamma\n",
    "        resultados.loc[idx,'param_reg'] = param_reg\n",
    "        resultados.loc[idx,'error de prueba (promedio)'] = np.mean(...)\n",
    "        resultados.loc[idx,'error de prueba (intervalo de confianza)'] = np.std(errores_test)\n",
    "        resultados.loc[idx,'# de vectores de soporte'] = np.mean(...)\n",
    "        resultados.loc[idx,'% de vectores de soporte'] = np.mean(...)\n",
    "        \n",
    "        idx+=1\n",
    "    return (resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB4bMhUhhTpr"
   },
   "outputs": [],
   "source": [
    "GRADER.run_test(\"ejercicio2\", experiementarSVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aatsj0EQhTpt"
   },
   "source": [
    "Para entrenar vamos a ignorar las dos primeras variables, estas corresponden a valores de fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRHwQhR7hTpu"
   },
   "outputs": [],
   "source": [
    "# vamos a realizar los experimentos\n",
    "resultadosSVR = experiementarSVR(x =X[:,2:],y=Y,\n",
    "                                 kernels=['linear', 'rbf'],\n",
    "                                 gammas = [0.01,0.1],\n",
    "                                 params_reg = [0.1, 1.0,10]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEPmKLZhhTpw"
   },
   "outputs": [],
   "source": [
    "# ver los 5 primeros resultados\n",
    "resultadosSVR.sort_values('error de prueba (promedio)',ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qERAELnQhTpx"
   },
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown ¿Cual es el objetivo de las las funciones kernel? Contestar dentro del contexto de las máquinas de sporte vectorial\n",
    "respuesta_1 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown Explique en sus palabras ¿qué representan los vectores de soporte en un problema de regresión?\n",
    "respuesta_2 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X37FqhDQhTp0"
   },
   "source": [
    "Para analizar los resultados vamos a crear dos graficas para el mejor modelo encontrado:\n",
    "1. vamos a graficar en el eje x el valor real, en el eje y el valor predicho. El modelo ideal deberia ser una recta que recuerda la identidad\n",
    "2. en el eje x vamos a dejar un valor incremental y con colores vamos a diferenciar entre el valor real y el valor predicho\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S6KsCkihTp1"
   },
   "outputs": [],
   "source": [
    "# dividir el conjunto\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#predicciones\n",
    "# OJO: Reemplazar los valores!\n",
    "Ypred =  predict_svr(X_train,y_train,X_test,kernel = ..., )\n",
    "\n",
    "# plots\n",
    "\n",
    "f, ax = plt.subplots(ncols=2, sharex=False, sharey=False, figsize = (22,6))\n",
    "ax[0].scatter(y_test, Ypred)\n",
    "ax[0].set_xlabel('valor real', fontdict = {'fontsize': 12})\n",
    "ax[0].set_ylabel('valor predicho', fontdict = {'fontsize': 12})\n",
    "ax[1].plot(y_test, label = 'valor real', color = 'b', alpha = 0.7)\n",
    "ax[1].plot(Ypred, label = 'valor predicho', color = 'r', alpha = 0.5)\n",
    "ax[1].legend()\n",
    "ax[1].set_ylabel('Humedad relativa', fontdict = {'fontsize': 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0wploEQHhTp3"
   },
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown usando las anteriores graficas, ¿como calificaria el modelo de manera cualitativa?.\n",
    "respuesta_3 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzJklaaBhTp5"
   },
   "source": [
    "### Ejercicio 3: Experimentar SVM para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDd9cs8YhTp5"
   },
   "source": [
    "En este ejercicio vamos a volver a resolver el problema de clasificación de dígitos. Vamos usar solo 5 clases y realizaremos un pre-procesamiento:\n",
    "1. mediante PCA (una tecnica proxima a practicar en el laboratorio)\n",
    "2. Vamos a convertirlo en problema biclase (vamos a diferenciar entre 0, 1 y el resto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK18YvtnhTp5"
   },
   "outputs": [],
   "source": [
    "Xcl, Ycl = load_digits(n_class=5,return_X_y=True)\n",
    "#--------- preprocesamiento--------------------\n",
    "pca = PCA(0.99, whiten=True)\n",
    "Xcl = pca.fit_transform(Xcl)\n",
    "# cambiar problema de clases\n",
    "unique, counts  = np.unique(Ycl, return_counts=True)\n",
    "print(\"distribución original (claves las etiquetas, valores el número de muestras): \\n\", dict(zip(unique, counts )))\n",
    "Ycl = np.where(np.isin(Ycl, [0,1]), Ycl, 2)\n",
    "unique, counts  = np.unique(Ycl, return_counts=True)\n",
    "print(\"Nueva distribución  (claves las etiquetas, valores el número de muestras): \\n\", dict(zip(unique, counts )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9AsO_TrhTp7"
   },
   "source": [
    "Ahora vamos a crear la función para experimentar con la maquina de soporte vectorial. Para ellos vamos:\n",
    "\n",
    "1. Usar la libreria de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "2. Vamos a variar tres parámetros del SVC: kernel,  gamma y el parametro de regularización.\n",
    "3. Utilizar la metodología cross-validation con 4 folds más adecuada para problemas de clasificación.\n",
    "4. Usar normalización de datos estandar implementada por sklearn\n",
    "5. Extraer los vectores de soporte (observe los *atributos* del modelo SVC de sklearn). Recuerde que estos atributos son accesibles una vez el modelo es entrenado\n",
    "6. vamos a probar la dos estragegias del SVC:\n",
    "    - One Vs One\n",
    "    - One Vs Rest\n",
    "7. Utilizar como error el score de exactitud de la clasificación de sklearn.\n",
    "\n",
    "**Notas**: \n",
    "- Deberiamos poder acceder a las funciones de la libreria de sklearn directamente por el nombre sin necesidad de importarlas. Las funciones que deberios utilizar ya están precargadas en la configuración del laboratorio.\n",
    "- Llame todos los parametros de las funciones de sklearn de manera explicita. (i.e, si se quiere usar `max_iter` como parámetro para el SVC, debe crear el objeto: `SVC(max_iter = 100)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sklearn tiene unos \"wrappers\", que implementan estrategias para la clasificación multiclase, uno  de estos wrappers, implementa la estrategia one-vs-rest: https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html.\n",
    "\n",
    "* Un wrapper, es un esquema de diseño común para \"envolver\" librerias/funciones con caracteristicas similares y poder modificar ciertos comportamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHpENPahhTp7"
   },
   "outputs": [],
   "source": [
    "#ejercicio de código\n",
    "def experiementarSVC(x, y, kernels, gammas,params_reg, estrategia = 'ovo'):\n",
    "    \"\"\"función que realizar experimentos sobre un SVM para clasificación\n",
    "    \n",
    "    x: numpy.Array, con las caracteristicas del problema\n",
    "    y: numpy.Array, con la variable objetivo\n",
    "    kernels: List[str], lista con valores a pasar \n",
    "        a sklearn correspondiente al kernel de la SVM\n",
    "    gammas: List[float], lista con los valores a pasar a\n",
    "        sklean correspondiente el valor de los coeficientes para usar en el\n",
    "        kernel\n",
    "    params_reg: List[float], lista con los valores a a pasar a \n",
    "        sklearn para ser usados como parametro de regularización\n",
    "    estrategia: str, valor que puede ser ovo (para one vs one) o ovr \n",
    "        (para one vs rest)\n",
    "    \n",
    "    retorna: pd.Dataframe con las siguientes columnas:\n",
    "        - 3 columnas con los tres parametros: kernel, gamma, param de regularizacion\n",
    "        - error cuadratico medio en el cojunto entrenamiento (promedio de los 4 folds)\n",
    "        - error cuadratico medio en el cojunto test (promedio de los 4 folds)\n",
    "        - % de Vectores de Soporte promedio para los 4 folds (0 a 100)\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    kf = ...(n_splits=...)\n",
    "    # crear una lista con la combinaciones de los elementos de cada list\n",
    "    kernels_gammas_regs = list(itertools.product(kernels, gammas, params_reg))\n",
    "    resultados = pd.DataFrame()\n",
    "    \n",
    "    for params in kernels_gammas_regs:\n",
    "        kernel, gamma, param_reg = params\n",
    "        print(\"parametros usados\", params) # puede usar para ver los params\n",
    "        errores_train = []\n",
    "        errores_test = []\n",
    "        pct_support_vectors = []\n",
    "        \n",
    "        for train_index, test_index in kf...(...,...):\n",
    "            X_train, X_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]  \n",
    "            # normalizar los datos\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            svm = SVC(kernel = ..., gamma = ..., C = ...)\n",
    "            \n",
    "            # si es estrategia \"envolver\" a la svm\n",
    "            if estrategia =='ovr':\n",
    "                svm = ... (svm)               \n",
    "            \n",
    "            # Entrenar el modelo\n",
    "            svm...(X=X_train, y=y_train)\n",
    "            # calculo de errores\n",
    "            y_train_pred = svm...(X=X_train)\n",
    "            y_test_pred = svm...(X=X_test)\n",
    "            # error y pct de vectores de soporte\n",
    "            errores_train.append(accuracy_score(y_true = y_train, y_pred = y_train_pred))\n",
    "            errores_test.append(accuracy_score(y_true = y_test, y_pred = y_test_pred))\n",
    "            # contar muestras de entrenamiento\n",
    "            n_train = X_train.shape[0]\n",
    "            if estrategia == 'ovr':\n",
    "                # en esta estrategia se realizar una SVM por cada clase\n",
    "                # por lo tanto tenemos que acceder a cada una de la SVM\n",
    "                # lee la documentacion\n",
    "                num_vs = np.mean([len(svc...) for svc in svm....])\n",
    "                pct_vs = (num_vs/n_train)*100\n",
    "                \n",
    "            else:\n",
    "                # cuando es ovo solo tenemos una SVM\n",
    "                pct_vs = (len(svm...)/n_train)*100\n",
    "            pct_support_vectors.append(pct_vs)\n",
    "        \n",
    "        resultados.loc[idx,'kernel'] = kernel\n",
    "        resultados.loc[idx,'gamma'] = gamma\n",
    "        resultados.loc[idx,'param_reg'] = param_reg\n",
    "        resultados.loc[idx,'estrategia'] = ...\n",
    "        resultados.loc[idx,'error de entrenamiento'] = np.mean(errores_train)\n",
    "        resultados.loc[idx,'error de prueba'] = np.mean(errores_test)\n",
    "        resultados.loc[idx,'% de vectores de soporte'] = np.mean(...)\n",
    "        idx+=1\n",
    "    return (resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPY9emZGhTp-"
   },
   "outputs": [],
   "source": [
    "GRADER.run_test(\"ejercicio3\", experiementarSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la estrategia OVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLwkxs1fhTp_"
   },
   "outputs": [],
   "source": [
    "# vamos a realizar los experimentos\n",
    "resultadosSVC_ovr = experiementarSVC(x = Xcl,y=Ycl,\n",
    "                                 kernels=['linear', 'rbf'],\n",
    "                                 gammas = [0.01,0.1],\n",
    "                                 params_reg = [0.001, 0.01,0.1, 1.0,10],\n",
    "                                estrategia = 'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver los mejores modelos\n",
    "resultadosSVC_ovr.sort_values('error de prueba', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver la estrategia OVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a realizar los experimentos\n",
    "resultadosSVC_ovo = experiementarSVC(x = Xcl,y=Ycl,\n",
    "                                 kernels=['linear', 'rbf'],\n",
    "                                 gammas = [0.01,0.1],\n",
    "                                 params_reg = [0.001, 0.01,0.1, 1.0,10],\n",
    "                                estrategia = 'ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver los mejores modelos\n",
    "resultadosSVC_ovo.sort_values('error de prueba', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "YcGKqkW8hTqB"
   },
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown Explique en sus palabras ¿qué representan los vectores de soporte en un problema de clasificación?\n",
    "respuesta_4 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "737HUyUqhTqD"
   },
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown Segun el tipo de problema (enfocarse en la distribución de clases) ¿La metrica usada es la adecuada? ¿Cual otra metrica del modulo de sklearn podria ser usada?\n",
    "respuesta_5 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta2szRQRhTqE"
   },
   "outputs": [],
   "source": [
    "# ver la relación de parametro de regularización y los vectores de soporte\n",
    "import seaborn as sns\n",
    "ax= sns.relplot(data = resultadosSVC_ovo, x = 'param_reg', y = '% de vectores de soporte', kind = 'line', hue ='kernel', aspect = 1.5)\n",
    "ax.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pPGG4r9ShTqG"
   },
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown ¿qué relación observa entre el valor del parametro de regularización y los vectores de soporte? ¿como explica esta relación?\n",
    "respuesta_6 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnTq9RTehTqH"
   },
   "outputs": [],
   "source": [
    "GRADER.check_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dQ-WZCK-hTqJ"
   },
   "outputs": [],
   "source": [
    "#@title Integrantes\n",
    "codigo_integrante_1 ='' #@param {type:\"string\"}\n",
    "codigo_integrante_2 = ''  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va7hyAs6hTqK"
   },
   "source": [
    "----\n",
    "esta linea de codigo va fallar, es de uso exclusivo de los profesores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ek0GsP_0hTqL"
   },
   "outputs": [],
   "source": [
    "GRADER.grade()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab5_parte2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
